# dbt-ecommerce-analytics

**Um projeto de pipeline de dados completo usando DBT, Docker e Postgres para transformar dados brutos e "sujos" de e-commerce em KPIs confi√°veis e prontos para an√°lise.**

---

## üéØ O Problema: A Dor que Solucionamos

No cen√°rio inicial, um analista de dados recebe tr√™s arquivos CSV de fontes diferentes e uma tarefa simples: "Qual √© a nossa receita total por produto?"

O analista n√£o consegue responder a essa pergunta em minutos. Ele passar√° dias sofrendo, pois:

* **Os Dados s√£o Sujos:** Os pre√ßos s√£o strings (`"R$ 3.999,90"`, `"3150.00"`), exigindo limpeza manual e complexa.
* **Os Dados n√£o s√£o Confi√°veis:** Encontramos pre√ßos negativos e IDs de produtos duplicados nas fontes.
* **Os Dados n√£o s√£o Integrados:** O `id_produto` da TechLoja n√£o √© o mesmo do GadgetPlace. O EAN/Barcode √© a √∫nica chave, mas os nomes dos produtos diferem.
* **O Processo n√£o √© Replic√°vel:** O analista faria essa limpeza em um script Python ou em um Excel. Se os CSVs mudarem amanh√£, todo o trabalho manual precisa ser refeito.

> **A Dor:** O tempo do analista √© gasto em **limpeza de dados**, em vez de **gera√ß√£o de insights**. A empresa n√£o tem confian√ßa nos n√∫meros.

---

## üöÄ A Solu√ß√£o: Uma F√°brica de Dados Confi√°veis

Este projeto n√£o √© apenas um script SQL. √â uma **f√°brica de dados (pipeline)** que transforma automaticamente o "lixo" da fonte em "ouro" anal√≠tico.

![Diagrama de fluxo do DBT](./.assets/dbt.png)

A solu√ß√£o pega os CSVs brutos e entrega tabelas limpas, testadas e prontas para o consumo (`marts`). Quando o analista acessa o banco de dados, ele n√£o v√™ a sujeira. Ele v√™ apenas as tabelas finais:

* `dim_produtos`: Uma lista mestra de produtos, limpa e com nomes padronizados.
* `fct_estoque_precos`: Um hist√≥rico de pre√ßos e estoque.
* `kpi_vendas_por_produto`: Uma tabela pr√©-calculada com a receita bruta por produto.

O analista agora responde a pergunta em **minutos**, n√£o em dias.

### Como o DBT resolveu isso?

* **Testes (`dbt test`):** O pipeline **para automaticamente** se dados sujos (como pre√ßos negativos ou IDs duplicados) forem detectados na fonte. Para garantir que apenas dados v√°lidos cheguem ao analista.
    
    ![Resultado dos Testes no Terminal](./.assets/dbt-test.png)

* **Transforma√ß√£o em Camadas (`staging`, `marts`):** Isolei a l√≥gica. A camada `staging` limpa a sujeira. A camada `marts` constr√≥i os modelos de neg√≥cio.

* **Performance (`incremental`):** O hist√≥rico de pre√ßos (`fct_`) n√£o √© reconstru√≠do do zero. Usei materializa√ß√£o incremental para processar **apenas** os dados novos, tornando o pipeline escal√°vel.

* **Documenta√ß√£o (`dbt docs`):** Gerei um site de documenta√ß√£o vivo que mostra a linhagem dos dados (de onde v√™m e para onde v√£o) e o que cada coluna significa.
    
    ![Gr√°fico de Linhagem do Projeto (dbt docs)](./.assets/data_lineage.png)
    ![Documenta√ß√£o do Projeto (dbt docs)](./.assets/dbt-doc.png)
---

## üõ†Ô∏è Stack de Tecnologia

* **Containeriza√ß√£o:** Docker & Docker Compose
* **Banco de Dados (Warehouse):** Postgres
* **Transforma√ß√£o & Testes:** DBT (Data Build Tool)